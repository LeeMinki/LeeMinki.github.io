---
layout: post
title: "나의 첫 딥러닝"
excerpt_separator:  <!--more-->
categories:
 - Machine Learning
tags:
  - 기계학습
  - 모두의 딥러닝
---

# 출처

## 조태호, 『모두의 딥러닝』, (주) 도서출판 길벗(2018-06-08), 18-39p

---

<!--more-->

## 01 최고급 요리를 먹을 시간

### 머신러닝(machine learning)

* 사람과 유사한 판단을 컴퓨터가 할 수 있게 하는 가장 효과적인 기법

### 딥러닝

* 머신러닝의 여러 알고리즘들 중 가장 효과적인 알고리즘
* 인공지능 = 음식
* 머신러닝 = 고기
* 딥러닝 = 최고급 스테이크

---

### 01-1 딥러닝 실행을 위한 준비 사항

#### 아나콘다 설치 --> 텐서플로 설치 --> 케라스 설치 --> 파이참 설치

#### 1. [아나콘다 설치](https://www.anaconda.com/download/)

#### 2. Anaconda Prompt 실행

#### 3. `conda create -n tutorial python=3.5 numpy scipy matplotlib spyder pandas seaborn scikit-learn h5py`

* tutorial --> 작업 환경 이름
* python=3.5 --> 파이썬 버전
* numpy ~ h5py --> 필요한 모든 라이브러리 이름

#### 4. `activate tutorial`

생성한 tutorial 환경 활성화 명령

#### 5. `pip install tensorflow`

텐서플로 설치

#### 6. `python`

파이썬 실행

#### 7. `import tensorflow as tf`

#### 8. `print(tf.__version__)`

텐서플로 버전 출력 시 텐서플로 설치 완료

#### 9. `exit()`

#### 10. `pip install keras`

케라스 설치

#### 11. [파이참 설치](https://www.jetbrains.com/pycharm/download/#section=windows)

#### 12. 파이참 실행

#### 13. Create New Project 버튼

#### 14. 경로 뒤에 \deeplearning 입력

#### 15. Project Interpreter... 클릭

#### 16. Existing interpreter 선택 후 오른쪽 끝 ... 클릭

#### 17. Add Local 선택 후 Conda Environment 선택 후 오른쪽 ... 클릭

#### 18. ![image](https://user-images.githubusercontent.com/28076542/44796353-c96da000-abe7-11e8-92e8-03b2e1afe933.png)

위와 같이 경로 입력 후 OK 클릭

#### 19. Create 버튼 클릭

#### 20. 윈도 탐색기로 PycharmProjects 폴더에 deeplearning 폴더 확인

#### 21. [폴더안에 예제 소스 파일 복사](http://www.gilbut.co.kr/book/bookView.aspx?bookcode=BN001909&page=1&TF=T)

![image](https://user-images.githubusercontent.com/28076542/44796561-2bc6a080-abe8-11e8-9a28-0ad09ffd78ea.png)

#### 22. ![image](https://user-images.githubusercontent.com/28076542/44796643-5e709900-abe8-11e8-8777-cc108bb77d3d.png)

#### 23. deep_code 폴더 > 01_My_First_Deeplearning.py 선택

#### 24. 메뉴의 Run > Run 클릭 --> 여기서 interpreter 설정 또 할 수도 있음

#### 25. 실행 결과 확인

```bash
C:\Users\kai01\Anaconda3\envs\tutorial\python.exe C:/Users/kai01/PycharmProjects/deeplearning/deep_code/01_My_First_Deeplearning.py
Using TensorFlow backend.
Epoch 1/30
2018-08-30 00:09:53.995181: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2

 10/470 [..............................] - ETA: 7s - loss: 1.0000 - acc: 0.0000e+00
470/470 [==============================] - 0s 386us/step - loss: 0.6611 - acc: 0.3149
Epoch 2/30

 10/470 [..............................] - ETA: 0s - loss: 0.0994 - acc: 0.9000
470/470 [==============================] - 0s 64us/step - loss: 0.1488 - acc: 0.8511
Epoch 3/30

 10/470 [..............................] - ETA: 0s - loss: 0.1000 - acc: 0.9000
470/470 [==============================] - 0s 72us/step - loss: 0.1488 - acc: 0.8511
Epoch 4/30

 10/470 [..............................] - ETA: 0s - loss: 0.3980 - acc: 0.6000
470/470 [==============================] - 0s 66us/step - loss: 0.1488 - acc: 0.8511
Epoch 5/30

 10/470 [..............................] - ETA: 0s - loss: 2.2996e-12 - acc: 1.0000
470/470 [==============================] - 0s 64us/step - loss: 0.1488 - acc: 0.8511
Epoch 6/30

 10/470 [..............................] - ETA: 0s - loss: 0.2000 - acc: 0.8000
470/470 [==============================] - 0s 59us/step - loss: 0.1487 - acc: 0.8511
Epoch 7/30

 10/470 [..............................] - ETA: 0s - loss: 0.1000 - acc: 0.9000
470/470 [==============================] - 0s 62us/step - loss: 0.1487 - acc: 0.8511
Epoch 8/30

 10/470 [..............................] - ETA: 0s - loss: 0.2000 - acc: 0.8000
470/470 [==============================] - 0s 59us/step - loss: 0.1487 - acc: 0.8511
Epoch 9/30

 10/470 [..............................] - ETA: 0s - loss: 5.3299e-07 - acc: 1.0000
470/470 [==============================] - 0s 64us/step - loss: 0.1487 - acc: 0.8511
Epoch 10/30

 10/470 [..............................] - ETA: 0s - loss: 0.1000 - acc: 0.9000
470/470 [==============================] - 0s 70us/step - loss: 0.1486 - acc: 0.8511
Epoch 11/30

 10/470 [..............................] - ETA: 0s - loss: 0.2000 - acc: 0.8000
470/470 [==============================] - 0s 76us/step - loss: 0.1498 - acc: 0.8447
Epoch 12/30

 10/470 [..............................] - ETA: 0s - loss: 0.2000 - acc: 0.8000
470/470 [==============================] - 0s 74us/step - loss: 0.1486 - acc: 0.8511
Epoch 13/30

 10/470 [..............................] - ETA: 0s - loss: 0.1000 - acc: 0.9000
470/470 [==============================] - 0s 68us/step - loss: 0.1485 - acc: 0.8511
Epoch 14/30

 10/470 [..............................] - ETA: 0s - loss: 0.2000 - acc: 0.8000
470/470 [==============================] - 0s 70us/step - loss: 0.1483 - acc: 0.8511
Epoch 15/30

 10/470 [..............................] - ETA: 0s - loss: 0.1000 - acc: 0.9000
470/470 [==============================] - 0s 59us/step - loss: 0.1485 - acc: 0.8511
Epoch 16/30

 10/470 [..............................] - ETA: 0s - loss: 0.1015 - acc: 0.9000
470/470 [==============================] - 0s 68us/step - loss: 0.1490 - acc: 0.8447
Epoch 17/30

 10/470 [..............................] - ETA: 0s - loss: 2.0702e-15 - acc: 1.0000
470/470 [==============================] - 0s 66us/step - loss: 0.1479 - acc: 0.8489
Epoch 18/30

 10/470 [..............................] - ETA: 0s - loss: 0.2000 - acc: 0.8000
470/470 [==============================] - 0s 74us/step - loss: 0.1482 - acc: 0.8468
Epoch 19/30

 10/470 [..............................] - ETA: 0s - loss: 0.3000 - acc: 0.7000
470/470 [==============================] - 0s 66us/step - loss: 0.1476 - acc: 0.8511
Epoch 20/30

 10/470 [..............................] - ETA: 0s - loss: 0.1000 - acc: 0.9000
470/470 [==============================] - 0s 62us/step - loss: 0.1480 - acc: 0.8511
Epoch 21/30

 10/470 [..............................] - ETA: 0s - loss: 0.2997 - acc: 0.7000
470/470 [==============================] - 0s 68us/step - loss: 0.1475 - acc: 0.8511
Epoch 22/30

 10/470 [..............................] - ETA: 0s - loss: 0.1000 - acc: 0.9000
470/470 [==============================] - 0s 64us/step - loss: 0.1469 - acc: 0.8511
Epoch 23/30

 10/470 [..............................] - ETA: 0s - loss: 0.1000 - acc: 0.9000
470/470 [==============================] - 0s 64us/step - loss: 0.1466 - acc: 0.8511
Epoch 24/30

 10/470 [..............................] - ETA: 0s - loss: 0.2083 - acc: 0.8000
470/470 [==============================] - 0s 70us/step - loss: 0.1475 - acc: 0.8489
Epoch 25/30

 10/470 [..............................] - ETA: 0s - loss: 0.2996 - acc: 0.7000
470/470 [==============================] - 0s 57us/step - loss: 0.1470 - acc: 0.8511
Epoch 26/30

 10/470 [..............................] - ETA: 0s - loss: 2.6178e-05 - acc: 1.0000
470/470 [==============================] - 0s 57us/step - loss: 0.1466 - acc: 0.8511
Epoch 27/30

 10/470 [..............................] - ETA: 0s - loss: 0.3000 - acc: 0.7000
470/470 [==============================] - 0s 57us/step - loss: 0.1472 - acc: 0.8511
Epoch 28/30

 10/470 [..............................] - ETA: 0s - loss: 0.2000 - acc: 0.8000
470/470 [==============================] - 0s 59us/step - loss: 0.1471 - acc: 0.8511
Epoch 29/30

 10/470 [..............................] - ETA: 0s - loss: 0.1991 - acc: 0.8000
470/470 [==============================] - 0s 57us/step - loss: 0.1470 - acc: 0.8489
Epoch 30/30

 10/470 [..............................] - ETA: 0s - loss: 8.9039e-07 - acc: 1.0000
470/470 [==============================] - 0s 55us/step - loss: 0.1461 - acc: 0.8532

 32/470 [=>............................] - ETA: 0s
470/470 [==============================] - 0s 68us/step

 Accuracy: 0.8511

Process finished with exit code 0
```

---

# 02 처음 해 보는 딥러닝

## 02-1 미지의 일을 예측하는 힘
